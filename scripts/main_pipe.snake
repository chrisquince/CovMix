from common import fill_default_values, detect_reads, extended_glob, assess_samples, assess_nanopore_samples
from os.path import abspath, realpath, dirname, basename, exists, getsize
from Bio.SeqIO.QualityIO import FastqGeneralIterator as fgi
from Bio.SeqIO.FastaIO import SimpleFastaParser as sfp
from collections import defaultdict
import numpy as np
import hashlib
import glob
import gzip
import sys
import os



# get defaults values 
fill_default_values(config)

# -------- Ressources --------
THREADS = config["threads"]
SCRIPTS = config["scripts"]
CONFIG_PATH = config["CONFIG_PATH"]
REPOS_DIR = config["REPOS_DIR"]
EXEC_DIR = config["EXEC_DIR"]

# -------- References/DB --------
PRIMER = config["PRIMER"]
DB_REF = "%s/primer_schemes/%s/%s_reference.fasta"%(REPOS_DIR,PRIMER,PRIMER) # ref for reference, basically the genome used for primer definition
DB_PRIMER = "%s/primer_schemes/%s/%s_primer.bed"%(REPOS_DIR,PRIMER,PRIMER)
DB_REP = config["database"]["fasta"] # rep for representatives, basically the covid genomes database

# ----------------- Amplicons -------------
# !!!ISSUE!!! currently dependent on artic type amplicon naming scheme, not sure how to adapt to general naming scheme. 
AMPLICONS = {line.rstrip().split("\t")[3].split("_RIGHT")[0].split("_LEFT")[0] for line in open(DB_PRIMER)}

# if pipe has been run previously RELEVANT_AMP is the list of selected amp to run for each sample
# otherwise just a placeholder so that the snaekamke doesn't crash by trying to make sense of an unknown variable
amp_selected = "%s/selected_amp.tsv"%EXEC_DIR
if exists(amp_selected):
    RELEVANT_AMP = {line.rstrip().split("\t")[0]:line.rstrip().split("\t")[1:] for line in open(amp_selected)}
else:
    RELEVANT_AMP = defaultdict(lambda:list(AMPLICONS))


# -------- Additional parameters --------
TRIMMING_LEVEL = config["trimming_strictness"]
THRESHOLD = config["Proportion_Threshold"]
DB_MSA = config["database"]["msa"]
DB_TREE = config["database"]["tree"]
DB_TREE_REL = config["database"]["tree_rel"]
DB_AMP = config["database"]["amplicons"]
MAX_READS = config["max_reads_per_amp"]
MIN_READS = config["min_reads_per_amp"]
MIN_PID = 0.95
MIN_CNT = 10

# --------- READS TYPE ------------
DATATYPE = config["DATATYPE"] # either ont or p-reads

# -------- Database --------
# we make the assumption that the database contain the sequence used for defining primer
# if it's not in, we add it.
refname,refseq = next(sfp(open(DB_REF))) 
if not refname in {header for header,seq in sfp(open(DB_REP))}:
    new_db = "%s/refs/genomes_database.fa"%EXEC_DIR
    os.system("mkdir -p %s/refs"%EXEC_DIR)
    os.system("cat %s %s > %s"%(DB_REP,DB_REF,new_db))
    DB_REP = new_db

if DB_MSA!="%s/refs/tree/db_mafft.msa"%EXEC_DIR:
    if not exists("%s/refs/tree/db_mafft.msa"%EXEC_DIR):
        # check that all element of ref can be found in the .msa
        headers_msa = {header.split(" ")[0] for header,_ in sfp(open(DB_MSA))}
        header_db = {header.split(" ")[0] for header,_ in sfp(open(DB_REP))}
        assert(header_db-headers_msa==set()),"Given multi alignement of the database is missing some entries, see for instance %s"%"\t".join(list(header_db-headers_msa))
        os.system("mkdir -p %s/refs/tree"%EXEC_DIR)
        os.system("ln -s %s %s/refs/tree/db_mafft.msa"%(DB_MSA,EXEC_DIR))
        DB_MSA = "%s/refs/tree/db_mafft.msa"%EXEC_DIR

# -------- filtering --------
# TODO check validity of passed files
# default is to run all amplicons
if config["amplicon_to_run"]:
    DB_RUN_A = config["amplicon_to_run"]
else:
    DB_RUN_A = ""

# default is to run all genomes
if config["genome_to_run"]:
    DB_RUN_G = config["genome_to_run"]
else:
    DB_RUN_G = ""


# -------- Samples --------
SAMPLE_DIR = config["data"]
try : 
    REGEX = config["data_regex"]
except:
    REGEX = ["*"]

SAMPLES = {basename(file):detect_reads(file) for regex in REGEX for file in extended_glob("%s/%s"%(SAMPLE_DIR,regex))}
SAMPLES = {sample:files for sample,files in SAMPLES.items() if files}

if len(RELEVANT_AMP)!=0:
    SAMPLES = {key:val for key,val in SAMPLES.items() if key in RELEVANT_AMP}

# check there is only 1 R1 and 1 R2 per folder
if DATATYPE=="p-reads":
    assess_samples(SAMPLES)
    R1 = {sample:files["_R1" in files[1]] for sample,files in SAMPLES.items()}
    R2 = {sample:files["_R2" in files[1]] for sample,files in SAMPLES.items()}
else: 
    assess_nanopore_samples(SAMPLES)
    SAMPLES = {sample:files[0] for sample,files in SAMPLES.items()}

# -------- misc python utils --------
def matrix_write(matrix,file_name,row_names,col_names) :
    with open(file_name,"w") as handle:
        handle.write("/\t%s\n"%"\t".join(col_names))
        handle.writelines('%s\t%s\n'%(row_names[index],"\t".join(["{0:4g}".format(nb) for nb in line])) for index,line in enumerate(matrix))


def load_matrix(file,sep="\t",sample_order=None,strain_order=None) :
    with open(file) as handle :
        header = next(handle).rstrip().split(sep)[1:]
        strains = []
        matrix = []
        for line in handle : 
            splitlines = line.rstrip().split(sep)
            strains.append(splitlines[0])
            matrix.append(list(map(float,splitlines[1:])))
    matrix = np.array(matrix)
    if sample_order :
        reorder_samples = [header.index(sample) for sample in sample_order]
        reorder_strain = [strains.index(strain) for strain in strain_order]
        return matrix[:,reorder_samples][reorder_strain,:]
    else : 
        return matrix,header,strains


# --------------------------------------------------
# ---------------- Snakemake start! ----------------
# --------------------------------------------------


rule results:
    input: expand("%s/results/{fig}.pdf"%EXEC_DIR,fig = ["Proportions","amplicons_wise_freq","tree_freq","amplicon_wise_read_cnt"]),
           "%s/results/proportions.tsv"%EXEC_DIR,
           "%s/snv/snv_table.tsv"%EXEC_DIR,
           expand("%s/samples/{sample}/varscan/{sample}_snv.tsv"%EXEC_DIR,sample=SAMPLES),

wildcard_constraints:
    amp ="|".join(AMPLICONS),
    sample = "|".join(SAMPLES)

# --------------- cut refferences amplicon-wise ------------------------
# build the tree
rule mafft_tree:
    threads: THREADS
    output: DB_MSA
    shell: "mafft --auto --thread {threads} {DB_REP} > {output}"

rule iqtree_building:
    input: DB_MSA
    params : "%s/refs/tree/db_tree.nwk"%EXEC_DIR
    output: DB_TREE
    log: "%s/refs/tree/iqtree.log"%EXEC_DIR
    threads:THREADS
    shell:"""
          iqtree -s {input} -m GTR+G -blmin 1e-9 -nt AUTO -ntmax {threads}  > {log}
          mv {input}.treefile {output}
          """

# build amplicon wise database
rule extract_amplicons:
    input: DB_MSA
    output: expand("%s/{amp}.fa"%DB_AMP,amp=AMPLICONS),
            "%s/amp_name_mapping.tsv"%DB_AMP
    shell: """
        {SCRIPTS}/extract_amplicons.py {DB_PRIMER} {input} {DB_AMP} -s {EXEC_DIR}/refs  """


# just symlink all relevant files in the refs folder.
rule create_tidy_ref_folder:
    input:"%s/amp_name_mapping.tsv"%DB_AMP
    output: "%s/refs/{PRIMER}_amps_seqs/tidy_done"%EXEC_DIR
    shell:"""
        mkdir -p {EXEC_DIR}/refs
        ln -sf {DB_PRIMER} {EXEC_DIR}/refs/
        ln -sf {DB_REF} {EXEC_DIR}/refs/        
        if [ ! -f {EXEC_DIR}/refs/genomes_database.fa ]
            then
                ln -sf {DB_REP} {EXEC_DIR}/refs/
        fi
        if [ ! -d {EXEC_DIR}/refs/{PRIMER}_amps_seqs ]
        then
            mkdir -p {EXEC_DIR}/refs/{PRIMER}_amps_seqs
            ln -sf {DB_AMP}/* {EXEC_DIR}/refs/{PRIMER}_amps_seqs/
        fi
        if [ ! -f {EXEC_DIR}/refs/tree/db_tree.nwk ]
        then  
            if [ -f {DB_TREE} ]
            then
                mkdir -p {EXEC_DIR}/refs/tree
                ln -sf {DB_TREE} {EXEC_DIR}/refs/tree/db_tree.nwk
            fi
        fi
        touch {output}
        """

# --------------- map reads to reference ------------------------

rule index :
    input: "{path}" 
    output:"{path}.pac"
    shell: "bwa index {input}"

if DATATYPE=="p-reads":

    rule map_init:
        input: "%s.pac"%DB_REF,
               R1 = lambda w:R1[w.sample],
               R2 = lambda w:R2[w.sample],
        output: "{path}/{sample}/{sample}_init.sam"
        shell:"bwa mem {DB_REF} {input.R1} {input.R2} | samtools sort -n - -o {output}"

    # !!!ISSUE!!! : this script depends on LEFT/RIGTH tags in the primer bed file...
    rule filt_sam :
        input: sam = "{path}/{sample}/{sample}_init.sam",
        params: "{path}/{sample}/amplicons_reads/{sample}",
        log: "{path}/{sample}/trim.log"
        output: files_R1 = expand("{{path}}/{{sample}}/amplicons_reads/{{sample}}_{amp}_{R}.fastq.gz",amp=AMPLICONS,R=["R1","R2"]),
                amp_tabl = "{path}/{sample}/{sample}_amp_cnt_init.tsv"
        shell: """
            {SCRIPTS}/trim_primer_parts.py {input.sam} {DB_PRIMER} {params} --table {output.amp_tabl} --log {log}
            """

    rule cat_trimmed_files :
        input: fq = expand("{{path}}/{{sample}}/amplicons_reads/{{sample}}_{amp}_{{R}}.fastq.gz",amp=AMPLICONS)
        params: "{path}/{sample}/amplicons_reads/",
        output: "{path}/{sample}/{sample}_trimmed_{R}.fastq.gz"
        shell: """
               cat {input.fq} > {output}
               """

    rule map_trimmed:
        input: R1 = "{path}/{sample}/{sample}_trimmed_R1.fastq.gz",
               R2 = "{path}/{sample}/{sample}_trimmed_R2.fastq.gz",
        output: "{path}/{sample}/{sample}_trimmed.sam"
        shell:"bwa mem {DB_REF} {input.R1} {input.R2} | samtools sort - -o {output}"



else:

    rule map_init:
        input: reads = lambda w:SAMPLES[w.sample],
        output: "{path}/{sample}/{sample}_init.sam"
        threads: 10
        shell:"minimap2  -t {threads} -ax map-ont {DB_REF} {input.reads} -a | samtools sort - -o {output}"

    rule filt_sam :
        input: sam = "{path}/{sample}/{sample}_init.sam",
        params: "{path}/{sample}/amplicons_reads/{sample}",
        log: "{path}/{sample}/trim.log"
        output: files= expand("{{path}}/{{sample}}/amplicons_reads/{{sample}}_{amp}.fastq.gz",amp=AMPLICONS),
                amp_tabl = temp("{path}/{sample}/{sample}_amp_cnt_init.tsv")
        shell: """
            {SCRIPTS}/trim_primer_parts.py {input.sam} {DB_PRIMER} {params} --table {output.amp_tabl} --log {log} --datatype ont
            """

    rule cat_trimmed_files :
        input: fq = expand("{{path}}/{{sample}}/amplicons_reads/{{sample}}_{amp}.fastq.gz",amp=AMPLICONS)
        params: "{path}/{sample}/amplicons_reads/",
        output: "{path}/{sample}/{sample}_trimmed.fastq.gz"
        shell: """
               cat {input.fq} > {output}
               """

    rule map_trimmed:
        input: reads = "{path}/{sample}/{sample}_trimmed.fastq.gz"
        output: "{path}/{sample}/{sample}_trimmed.sam"
        threads: 10
        shell:"minimap2 -t {threads} -ax map-ont {DB_REF} {input.reads} -a | samtools sort - -o {output}"

rule resort_init_sam:
    input: "{path}_init.sam"
    output: "{path}_init_sorted.sam"
    shell: "samtools sort {input} > {output}"

rule plot_cov :
    input: "{path}/samples/{sample}/{sample}_init_sorted.sam",
           "{path}/samples/{sample}/{sample}_trimmed.sam"
    output: "{path}/samples/{sample}/amplicon_wise_read_cnt.pdf"
    threads: 1
    shell : "{SCRIPTS}/plot_depth.py -o {output} -i {input} -p {DB_PRIMER} -t {threads}"

# -------------------------- merged version of the pipeline ----------------------
rule tabulate_amp_merged:
    input: cnts = "{path}/{sample}_amp_cnt_init.tsv",
           global_al = expand("{{path}}/amplicons_reads/{{sample}}_{amp}.m6",amp=AMPLICONS)
    output: cnt = "{path}/{sample}_amp_cnt.tsv"
    run:
        nb_amp = len(AMPLICONS)
        sorted_amp = sorted(AMPLICONS)
        sample = wildcards["sample"]
        matrix = np.zeros((1,nb_amp))
        amp_to_length = {amp:np.mean([len(seq) for header,seq in sfp(open("%s/refs/%s_amps_seqs/%s.fa"%(EXEC_DIR,PRIMER,amp)))]) for amp in sorted_amp}
        dummy = "need_at_least_one_entry_so_vsearch_doesnt_throw_an_error"
        for file in input["global_al"]:
            os.system("sed -i '/^%s/d' %s"%(dummy,file))
            os.system("sed -i '/^%s/d' %s"%(dummy,file.replace(".m6",".sam")))
            if getsize(file)!=0:
                amp = basename(file).split("%s_"%sample)[1].replace(".m6","")
                headers = {line.rstrip().split("\t")[0] for line in open(file)}
                nb_reads = sum([int(header.split(";size=")[1]) for header in headers])
                index_amp = sorted_amp.index(amp)
                matrix[0,index_amp]=int(nb_reads)
        # output cnts
        matrix_write(matrix,output["cnt"],[sample],map(str,sorted_amp))


rule select_sample_and_amp:
    input: cnts = expand("{{path}}/samples/{sample}/{sample}_amp_cnt.tsv",sample=SAMPLES),
           tidy = "%s/refs/%s_amps_seqs/tidy_done"%(EXEC_DIR,PRIMER),
    output: selec = "{path}/selected_amp.tsv",
            cnt = "{path}/results/amp_cnts.tsv"
    run:
        nb_amp = len(AMPLICONS)
        sorted_amp = sorted(AMPLICONS)
        sorted_samples = sorted([basename(dirname(file)) for file in input["cnts"]])
        matrix = np.zeros((len(input["cnts"]),nb_amp))
        amp_to_length = {amp:np.mean([len(seq) for header,seq in sfp(open("%s/refs/%s_amps_seqs/%s.fa"%(EXEC_DIR,PRIMER,amp)))]) for amp in sorted_amp}
        sample_to_amp = defaultdict(list)
        for file in input["cnts"]:
            sample = basename(dirname(file))
            index_sample = sorted_samples.index(sample)
            cnts,sorted_amp,_ = load_matrix(file,sep="\t")
            for index,amp in enumerate(sorted_amp):
                if cnts[0,index]>=MIN_READS:
                    sample_to_amp[sample].append(amp)
            matrix[index_sample,:] = cnts

        #output results
        with open(output['selec'],"w") as handle:
            handle.writelines("%s\t%s\n"%(sample,"\t".join(amps)) for sample,amps in sample_to_amp.items())
        matrix_write(matrix,output["cnt"],sorted_samples,sorted_amp)

            
# ---------------------- run vsearch ----------------------------

if DATATYPE=="p-reads":
    rule vsearch_mergepair:
        input: R1="{path}_R1.fastq.gz",
               R2="{path}_R2.fastq.gz"
        output:temp("{path}.merged")
        shell:"vsearch --threads 1 --fastq_mergepairs {input.R1} --reverse {input.R2} --fastq_minovlen 5 --fastaout {output} || touch {output}" # overlap is filtered for and all paired reads overlap at least by 1
    rule deal_with_low_overlap:
        input: merged = "{path}.merged"
        output:"{path}.todereplicate"
        run:
            # we do a manual merging of paired reads overlapping with 0-5 bases. I'll just keep the better qual one if bases are different
            # concat that to the vsearch merged ones.
            R1_handle = fgi(gzip.open("%s_no_ovlp_R1.fastq.gz"%wildcards['path'], "rt"))
            R2_handle = fgi(gzip.open("%s_no_ovlp_R2.fastq.gz"%wildcards['path'], "rt"))
            with open(output[0],"w") as handle:
                for h_R1,seq_R1,qual_R1 in R1_handle:
                    h_R2,seq_R2,qual_R2 = next(R2_handle)
                    assert(h_R1==h_R2),"issue with non overlaping reads order"
                    header,overlap = h_R1.split("_ovlp=")
                    overlap = int(overlap)
                    if (len(seq_R1)>overlap)&(len(seq_R2)>overlap):
                        if overlap==0:
                            handle.write(">%s\n%s\n"%(header,seq_R1+seq_R2))
                        else:
                            seq = seq_R1[:-overlap]
                            for i in range(0,overlap):
                                nuc1 = seq_R1[-overlap+i]
                                qual1 = ord(qual_R1[-overlap+i])-33
                                nuc2 = seq_R2[i]
                                qual2 = ord(qual_R2[i])-33
                                nuc = max([(nuc1,qual1),(nuc2,qual2)],key=lambda x:x[1])
                                seq += nuc[0]
                            seq += seq_R2[overlap:]
                            handle.write(">%s\n%s\n"%(header,seq))
                    else:
                        seq = max([seq_R1,seq_R2],key=len)
                        handle.write(">%s\n%s\n"%(header,seq))
                handle.write(open(input["merged"]).read())
else:
    rule fastq_to_fasta:
        input: "{path}.fastq.gz"
        output: temp("{path}.todereplicate")
        shell: "seqtk seq -a {input} > {output}"

rule dereplication:
    input: "{path}.todereplicate"
    output: temp("{path}.fasta")
    shell: "vsearch --derep_fulllength {input} --output {output} -sizeout"

rule subsample_reads:
    input: "{path}.fasta"
    output: "{path}_F%s.fasta"%MAX_READS
    shell: "seqtk sample -s42 {input} {MAX_READS} > {output}"

rule vsearch_all_merged:
    input: R = "{path}_{amp}_F%s.fasta"%MAX_READS,
           db = "%s/{amp}.fa"%DB_AMP,
    params: tmp = "{path}_{amp}.tmp",
    output: sam = "{path}_{amp}.sam",
            m6 = "{path}_{amp}.m6"
    shell: """
    sed 's/-//g' {input.db} > {params.tmp}
    sed 's/nCoV2019/nCoV-2019/g' -i {params.tmp}
    echo '>need_at_least_one_entry_so_vsearch_doesnt_throw_an_error'>>{input.R}
    echo 'ATGC'>>{input.R}
    vsearch --threads 1 --usearch_global {input.R} --db {params.tmp} --samout {output.sam} --samheader --id {MIN_PID} --maxaccepts 1000000 --userout {output.m6} --userfields query+target+evalue+id+pctgaps+pairs+gaps+qlo+qhi+tlo+thi+pv+ql+tl+qs+ts+alnlen+opens+exts+raw+bits+aln+caln+qrow+trow+mism+ids+qcov+tcov
    rm {params.tmp}
    """

#-------------------- Simple EM with error rate learning ------------------
rule cat_merged_m6:
    input: prior_step = "{path}/selected_amp.tsv",
           m6 = lambda w:["%s/samples/%s/amplicons_reads/%s_%s.m6"%(w.path,w.sample,w.sample,amp) for amp in RELEVANT_AMP[w.sample]],
           sam = lambda w:["%s/samples/%s/amplicons_reads/%s_%s.sam"%(w.path,w.sample,w.sample,amp) for amp in RELEVANT_AMP[w.sample]]
    output: fasta = "{path}/samples/{sample}/{sample}.fasta",
            m6 = temp("{path}/samples/{sample}/amplicons_reads/all_amp.m6"),
            sam = temp("{path}/samples/{sample}/amplicons_reads/{sample}_all_refs.sam")
    shell: """
           cat {input.m6} > {output.m6}
           cat {input.sam} > {output.sam}
           """


rule EMP_aggregated_merged:
    input: m6 = "{path}/samples/{sample}/amplicons_reads/all_amp.m6",
           mapping = "%s/amp_name_mapping.tsv"%DB_AMP
    output: expand("{{path}}/samples/{{sample}}/EM_runs/{{sample}}_total{ext}.csv",ext=["_pi_est", "_z_est","_amp_pi_est"])
    params: out = "{path}/samples/{sample}/EM_runs/{sample}_total",
            filt = (DB_RUN_A!="")*("-a "+DB_RUN_A+" ")+(DB_RUN_G!="")*("-r "+DB_RUN_G+" ")
    shell: """
           python  {SCRIPTS}/CovidEM_VIN.py {input.m6} {input.mapping} {params.out} {params.filt}
           touch {output}
           """



# -------------------- create a results figs ------------------
ruleorder: plot_proportions>plot_cov >plot_freq_tree>plot_Amplicons> merge_pdf

rule plot_proportions:
    input : "{path}/samples/{sample}/EM_runs/{sample}_total_pi_est.csv"
    output: "{path}/samples/{sample}/Proportions.pdf"
    shell:"{SCRIPTS}/plot_proportions.R {input} {THRESHOLD} {output}"

rule plot_Amplicons:
    input : "{path}/samples/{sample}/EM_runs/{sample}_total_amp_pi_est.csv"
    output: "{path}/samples/{sample}/amplicons_wise_freq.pdf"
    shell:"{SCRIPTS}/AmpZZ.R {input} {output}"

rule plot_freq_tree:
    input : pi = "{path}/samples/{sample}/EM_runs/{sample}_total_pi_est.csv",
            tree = DB_TREE
    output: "{path}/samples/{sample}/tree_freq.pdf"
    shell:"{SCRIPTS}/plot_tree.R {input.pi} {input.tree} {output}"

# -------------------- create a results folder ------------------
rule merge_pdf:
    input : lambda w:sorted(["%s/samples/%s/%s.pdf"%(w.path,sample,w.fig) for sample in RELEVANT_AMP],key=lambda x:basename(dirname(x))),
    output: "{path}/results/{fig}.pdf",
    shell: "pdfunite {input} {output}"

rule Pi_overall_table:
    input : lambda w:["%s/samples/%s/EM_runs/%s_total_pi_est.csv"%(w.path,sample,sample) for sample in RELEVANT_AMP],
    output: "{path}/results/proportions.tsv"
    run:
        refs = {line.rstrip().split(",")[1] for file in input for index,line in enumerate(open(file)) if index>0}
        sample_to_file = {basename(file).replace("_total_pi_est.csv",""):file for file in input}
        # some bad runs don't really have a filtered output since there is nothing left after filtering, we require the output of CovidEM_VIN to be only total_pi_est instead of total_Filt_pi_est but then when possible use the Filtered output 
        sample_to_file = {sample:file.replace("total_","total_Filt_") if os.path.exists(file.replace("total_","total_Filt_")) else file for sample,file in sample_to_file.items()}
        sorted_refs = sorted(refs)
        sorted_samples = sorted(sample_to_file)

        # build a matrix
        mat = np.zeros((len(input),len(refs)))
        for index,sample in enumerate(sorted_samples):
            with open(sample_to_file[sample]) as handle:
                _=next(handle)
                for line in handle:
                    _,ref,percent,std = line.rstrip().split(",")
                    mat[index,sorted_refs.index(ref)]=float(percent)

        # write a matrix
        matrix_write(mat,output[0],sorted_samples,sorted_refs)


# ----------------------------------------------------------------
# -------------------- SNV part of the pipeline ------------------
# ----------------------------------------------------------------
def generate_header():
    # we did a sam file concatenation, it's shit for headers,
    # let's look at what it should be and generate some correct ones
    # see that for md5 in python https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file
    def md5(seq):
        return hashlib.md5(seq.encode('utf-8')).hexdigest()

    refname,seq = next(sfp(open(DB_REF)))
    header=[]
    header.append(["@HD","VN:1.0","SO:unsorted","GO:query"])
    header.append(["@SQ","SN:%s"%refname,"LN:%s"%len(seq),"M5:%s"%md5(seq),"UR:file:%s"%DB_REF]) 
    header.append(["@PG","ID:vsearch","VN:2.17.1","CL:vsearch --threads 10 --usearch_global sample --db %s --samout samfile --samheader --id %s"%(refname,MIN_PID)])
    return header


def correct_line(line,amp_def):
    # need to add deletions to indicate where al starts
    splitline = line.split("\t")
    ref = splitline[2].split("_var")[0]
    amp_start,amp_end = amp_def[ref] # this is zero based
    new_start = str(int(splitline[3])+amp_start) # this is 1 based
    splitline[3] = new_start
    splitline[2] = refname
    # it is extremly likely you get a flag 256: not primary alignment from the fact we do an alignment to everyting. So, If a better ref exist, refname will never be the primary al.
    # this is not an issue save that bam readcount ignore anything 256
    # hence we change the sam flag to 0
    splitline[1] = str(0)
    return "\t".join([el for el in splitline if "MD:" not in el])


rule remove_other_refs:
    input: map = "%s/amp_name_mapping.tsv"%DB_AMP,
           sam = "{path}/{sample}_all_refs.sam"
    output: sam = "{path}/{sample}.sam"
    run:
        ref_names = {line.rstrip().split("\t")[1] for line in open(input["map"]) if refname in line}
        with open(output["sam"],"w") as handle:
            for line in open(input["sam"]):
                if line[0]=="@":
                    continue
                ref = line.split("\t")[2]
                if ref in ref_names:
                    handle.write(line)


rule fix_sam:
    input: sam = '{path}.sam'
    output: sam = "{path}_fixed.sam"
    run:
        amp_def = defaultdict(lambda:[0,0])
        for line in open(DB_PRIMER):
            splitline = line.rstrip().split("\t")
            primer = splitline[3]
            if "_alt" in primer:
                continue
            amp = primer.replace("_LEFT","").replace("_RIGHT","")
            if "_LEFT" in primer:
                amp_def[amp][0] = int(splitline[2])
            if "_RIGHT" in primer:
                amp_def[amp][1] = int(splitline[1])

        new_headers = generate_header()
        with open(input["sam"]) as handle:
            with open(output["sam"],"w") as handle_w:
                handle_w.writelines("%s\n"%"\t".join(line) for line in new_headers)
                for line in handle:
                    if line[0]=="@":
                        continue
                    else:
                        handle_w.write(correct_line(line,amp_def))

rule sam_to_bam:
    input: "{path}_fixed.sam" 
    output:"{path}.bam"
    shell: "samtools view -b -h {input}| samtools sort - > {output}"

rule ref_index:
    input: "{path}" 
    output:"{path}.fai"
    shell: "samtools faidx {input}"

rule bam_readcount:
    input:
        fai = "%s.fai"%DB_REF,
        bam = "{path}.bam",
    output: "{path}.cnt.gz"
    log:
        "{path}.log"
    shell: """
        samtools index {input.bam}
        bam-readcount -w 1 -f {DB_REF} {input.bam} 2> {log} | gzip > {output}
        """

rule build_amp_coordinates:
    output: coord = "{path}/ref_coord.tsv"
    run:
        with open(output["coord"],"w") as handle:
            handle.write("%s,%s,%s,%s,%s,1"%(refname,refname,0,len(refseq),refname))

rule extract_counts:
    input:
        expand("{{path}}/samples/{sample}/amplicons_reads/{sample}.cnt.gz", sample=SAMPLES),
        coords = "{path}/snv/ref_coord.tsv"
    output:
        "{path}/snv/count_raw.csv"
    params:
        input_dir = "{path}/snv/cnts"
    log:
        "{path}/snv/extract_counts.log"
    shell: """
    mkdir -p {wildcards.path}/snv/cnts
    ln -sf {input} {wildcards.path}/snv/cnts/
    python {SCRIPTS}/ExtractCountFreqGenes.py {input.coords} {params.input_dir} --output_file {output} &>> {log}
        """
rule filter_variants:
    input: "{path}/count_raw.csv"
    output: cnts = "{path}/count.csv",
            fold = "{path}/filter/counts_tran_df.csv"
    params: stub = "{path}/filter/counts_"
    shell:  """
    {SCRIPTS}/Variant_Filter.py {input} -o {params.stub} 
    mv {params.stub}sel_var.csv {output.cnts}
    """


rule plot_and_snv_table:
    input: "{path}/snv/count.csv",
           "{path}/refs/db_snv.tsv",
           "{path}/results/proportions.tsv",
    output: "{path}/snv/snv_table.tsv"
    shell: "{SCRIPTS}/snv_summary.py {EXEC_DIR}"


# -------------------------------------
# -------------- VARSCAN --------------
# -------------------------------------

rule mpilleup :
    input: "{path}/{sample}_trimmed.sam"
    output: "{path}/varscan/{sample}.pileup"
    shell: "samtools view -b {input} | samtools mpileup -f {DB_REF} - >{output}"

rule varscan :
    input: "{path}.pileup"
    output: snv = "{path}_snv.tsv",
            indel = "{path}_indel.tsv"
    shell: """
    varscan pileup2snp {input} > {output.snv}
    varscan pileup2indel {input} > {output.indel}
    """




# -------------------------------------
# ------ clean files ------------------ 
# -------------------------------------




